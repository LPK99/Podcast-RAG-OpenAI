0
00:00:6
OpenAI, a company that we all know now, but only a year ago was 100 people, is changing

1
00:00:11
the world.

2
00:00:12
Their research is leading the charge to AGI.

3
00:00:15
Since ChatGPT captured consumer attention last November, they show no signs of slowing

4
00:00:19
down.

5
00:00:20
This week, a lot of nice sit down with Ilya Sutskmer, co-founder and chief scientist

6
00:00:25
at OpenAI to discuss the state of AI research, where we'll hit limits, the future of AGI,

7
00:00:31
and what's going to take to reach super alignment.

8
00:00:34
Ilya, welcome to no priors.

9
00:00:36
Thank you.

10
00:00:36
Let's go to be here.

11
00:00:37
Let's start at the beginning.

12
00:00:38
Pre-AlexNet, nothing in deep learning was really working.

13
00:00:41
And then given that environment, you guys took a very unique bet, what motivated you to

14
00:00:46
go in this direction?

15
00:00:48
Indeed, in those dark ages, AI was not an area where people had hope and people who were

16
00:00:58
not accustomed to any kind of success at all.

17
00:00:61
And because there wasn't, there hasn't been any success, there was a lot of debate, and

18
00:00:66
there were different schools of thoughts that had different arguments about how machine

19
00:00:71
learning in AI should be.

20
00:00:73
You had people who were into knowledge representation from the good old fashioned AI.

21
00:00:78
You had people who were basians, and they liked Bayesian non-parametric methods.

22
00:00:83
You had people who like graphical models, and you had the people who like neural networks.

23
00:00:87
Those people were marginalised, because neural networks did not have the property that

24
00:00:93
you can't prove maths theorems about them.

25
00:00:96
If you can't prove theorems about something, it means that your research isn't good.

26
00:00:99
That's how it has been.

27
00:00:101
But the reason why I gravitated to neural networks from the beginning is because it felt

28
00:00:105
like those are small little brains.

29
00:00:107
And who cares if you can prove any theorems about them, because we are training small little

30
00:00:111
brains, and maybe they'll do something one day.

31
00:00:115
And the reason that we were able to do Alex, that when we did is because a combination of

32
00:00:121
two factors, three factors.

33
00:00:124
The first factor is that this was shortly after GPUs started to be used in machine learning.

34
00:00:130
So it kind of had an intuition that that's a good thing to do, but it wasn't like today

35
00:00:135
where people exactly knew what an GPU is for.

36
00:00:137
It was like, oh, let's play with those cool fast computers and see what we can do with

37
00:00:140
them.

38
00:00:141
It was an especially good fit for neural networks, so that was a very, that definitely helped

39
00:00:145
us.

40
00:00:146
I was very fortunate in that I was able to realise that the reason neural networks of the

41
00:00:154
time weren't good is because they were too small.

42
00:00:157
So if you try to solve a vision task with a neural network, which has like a thousand

43
00:00:163
neurons, what can it do?

44
00:00:164
It can't do anything.

45
00:00:165
It doesn't matter how good your learning is and everything else.

46
00:00:169
But if you have a much larger neural network, it will do something unprecedented.

47
00:00:172
Well, I gave you the intuition to think that that was the case, because I think at the

48
00:00:176
time it was reasonably, I'm contrarian to think that despite your point, you know, a lot

49
00:00:181
of the human brain in some sense works that way or different, you know, biological neural

50
00:00:184
circuits.

51
00:00:185
I'm curious like what gave you that intuition early on to think that this was a good direction.

52
00:00:189
I think, yeah, looking at the brain and specifically, if you, it like, have all those things

53
00:00:196
follow very easily, if you allow yourself, if you allow yourself to accept the idea, right

54
00:00:204
now this idea is reasonably well accepted.

55
00:00:207
Back then people still talked about it, but they haven't really accepted it or internalised.

56
00:00:212
The idea that maybe an artificial neuron in some sense is not a different from a biological

57
00:00:218
neuron.

58
00:00:219
So now, whatever you imagine animals do with their brains, you could perhaps assemble

59
00:00:224
some artificial neural network of similar size.

60
00:00:227
Maybe if you train it, it will do something similar.

61
00:00:230
So there, so that leads you to start to imagine, okay, like, you almost imagine the computation

62
00:00:238
being done by the neural network.

63
00:00:239
You can almost think like if you have a high resolution image and you have like one

64
00:00:245
neuron for like a large group of pixels, what can the neuron do?

65
00:00:248
It's just not much it can do if you, but if you have a lot of neurons, then they can actually

66
00:00:251
do something and compute something.

67
00:00:253
So I think it was like, alright, like it was, this was, it was considerations like this,

68
00:00:257
plus a technical realisation.

69
00:00:260
The technical realisation is that if you have a large training set that specifies the

70
00:00:269
behaviour of the neural network and the training set is large enough such that it can constrain

71
00:00:275
the large neural network sufficiently.

72
00:00:277
And furthermore, if you have the algorithm to find that neural network because what we

73
00:00:282
do is that we turn the training set into a neural network which satisfies the training set.

74
00:00:288
Neural network training can almost be seen as solving a neural equation.

75
00:00:296
Learning a neural equation where every data point is an equation and every parameter is

76
00:00:302
a variable.

77
00:00:304
And so it was multiple things.

78
00:00:307
The realisation at the bigger neural network could do something unprecedented.

79
00:00:312
The realisation that if you have a large data set together with the compute to solve the

80
00:00:319
neural equation, that's what gradient descent comes in, but it's not gradient descent.

81
00:00:324
Gradient descent was around for a long time.

82
00:00:326
It was certain technical insights about how to make it work because back then the prevailing

83
00:00:331
belief was, well, you can't train those neural nets anything, it's all hopeless.

84
00:00:334
So it wasn't just about the size, it was about even if someone did think, gosh, it would

85
00:00:339
be cool to train a big neural net, they didn't have the technical ability to turn this idea

86
00:00:345
into reality.

87
00:00:346
You needed not only to code the neural net, you need to do a bunch of things right and

88
00:00:350
only then it will work.

89
00:00:352
And then another fortunate thing is that the person who my work with Alex Krzyszywski,

90
00:00:357
he just discovered that he really loves GPUs and he was perhaps one of the first person

91
00:00:362
who really mastered writing really, really perform it code for the GPUs and that's why

92
00:00:370
we were able to squeeze a lot of performance out of two GPUs and do something and produce

93
00:00:374
something unprecedented.

94
00:00:375
So to sum up, it was multiple things.

95
00:00:379
The idea that a big neural network, in this case, a vision neural network, a convolutional

96
00:00:383
neural network with many layers, one that's much, much bigger than anything that's ever

97
00:00:388
been done before, could do something very unprecedented because the brain can see and

98
00:00:392
the brain is a large neural network and we can see quickly so a neural net has a lot of

99
00:00:397
time.

100
00:00:397
Then the computer needed the technical know how that in fact, we can't train such neural

101
00:00:403
networks and it was not at all widely distributed.

102
00:00:406
If people in machine learning would not have been able to train such a neural network

103
00:00:410
even if they wanted to.

104
00:00:412
Did you guys have any particular goal from a size perspective or was it just as biologically

105
00:00:421
inspired or where that number comes from or just as large as we can go?

106
00:00:424
Definitely as large as we can go.

107
00:00:426
Because keep in mind, we had a certain amount of computer which we could use fully consume

108
00:00:431
and then what can it do?

109
00:00:434
Maybe if we think about just like the origin of open AI and the goals of the organization,

110
00:00:441
what was the original goal and how is that evolved over time?

111
00:00:444
The goal did not evolve over time, the tactic evolved over time.

112
00:00:450
The goal of open AI from the very beginning has been to make sure that artificial general

113
00:00:458
intelligence by which remain autonomous systems, AI that can actually do most of the jobs

114
00:00:466
and activities and tasks that people do, benefits all of humanity.

115
00:00:471
That was the goal from the beginning.

116
00:00:473
The initial thinking has been that maybe the best way to do it is by just open sourcing

117
00:00:479
a lot of technology.

118
00:00:482
We later and we also attempted to do it as a nonprofit.

119
00:00:486
It seemed very sensible.

120
00:00:487
This is the goal.

121
00:00:489
Non-profit is the way to do it.

122
00:00:491
What changed?

123
00:00:492
Some point at open AI, we realized that we were, perhaps among the earlier, the earliest

124
00:00:499
to realize that to make progress in AI for real, you need a lot of compute.

125
00:00:505
Now, what does a lot mean?

126
00:00:506
The appetite for compute is truly endless as now clearly seen.

127
00:00:512
We realized that we will need a lot.

128
00:00:516
Non-profit wouldn't be the way to get there, wouldn't be able to build a large cluster

129
00:00:522
with a nonprofit.

130
00:00:522
That's why we became, we converted into this unusual structure called CAP-profit.

131
00:00:530
To my knowledge, we are the only CAP-profit company in the world.

132
00:00:533
The idea is that investors put in some money, but even if the company does incredibly well,

133
00:00:539
they don't get more than some multiplier on top of their original investment.

134
00:00:544
The reason to do this, the reason why that makes sense.

135
00:00:549
There are arguments, one could make arguments against it as well, but the argument for it

136
00:00:554
is that if you believe that the technology that we are building, AI, could potentially be

137
00:00:564
so capable as to do every single task that people do, does it mean that it might unemploy

138
00:00:571
everyone?

139
00:00:573
Well, I don't know, but it's not impossible.

140
00:00:575
And if that's the case, it makes sense, it will make a lot of sense if the company

141
00:00:579
that builds such a technology would not be able to make infinite, would not be incentivized

142
00:00:584
rather to make infinite profits.

143
00:00:586
I don't know if it will literally play out this way because of competition in AI.

144
00:00:591
There will be multiple companies and a thing that will have some on-for-seen implications

145
00:00:597
on the argument which I'm making, but that was the thinking.

146
00:00:599
I remember visiting the offices back when you were, I think, how's it YC or something

147
00:00:603
or cohabited some space there?

148
00:00:605
And at the time, there was a suite of different efforts.

149
00:00:609
There was robotic arms that were being manipulated.

150
00:00:612
And then there was some video game-related work, which was really cutting edge.

151
00:00:616
How did you think about how the research agenda evolved and what really drove it down

152
00:00:620
this path of transformer-based models and other forms of learning?

153
00:00:624
So I think it has been evolving over the years from when we started OpenAI.

154
00:00:631
And the first year we indeed did some of the more conventional machine learning work.

155
00:00:635
The conventional machine learning work, I mean, because the world has changed so much,

156
00:00:639
a lot of things which were known to everyone in 2016 or 2017 are completely and utterly

157
00:00:645
forgotten.

158
00:00:646
It's like the Stone Age almost.

159
00:00:648
So in that Stone Age, the world of machine learning looked very different.

160
00:00:653
It was dramatically more academic.

161
00:00:658
The goals, values and objectives were much more academic.

162
00:00:662
They were about discovering small bits of knowledge and sharing them with the other researchers

163
00:00:667
and getting scientific recognition as a result.

164
00:00:669
And it's a very valid goal and it's very understandable.

165
00:00:672
I've been doing AI for 20 years now.

166
00:00:674
More than half of my time that I spent in AI was in that framework.

167
00:00:677
And so what do you do?

168
00:00:679
You write papers, you share your small discoveries.

169
00:00:682
Two realizations.

170
00:00:682
The first realization is just at a high level, it doesn't seem like it's the way to go

171
00:00:688
to forward dramatic impact.

172
00:00:690
And why is that?

173
00:00:691
Because if you imagine how an AI should look like, it has to be some kind of a big engineering

174
00:00:699
project that's using a lot of compute, right?

175
00:00:703
Even if you don't know how to build it, what that should look like.

176
00:00:705
You know that this is the ideal you want to strive towards.

177
00:00:707
So you want to somehow move towards larger projects as opposed to small projects.

178
00:00:711
So while we attempted a first large project where we trained in neural network to play

179
00:00:718
a real time strategy game as well as the best humans.

180
00:00:722
It's the Dota 2 project and it was driven by two people.

181
00:00:727
Iyaka Pahotsky and Greg Brockman, they really drove this project and made it a success.

182
00:00:733
And this was our first attempt at a large project.

183
00:00:737
But it wasn't quite the right formula for us because the neural networks were a little

184
00:00:741
bit too small.

185
00:00:742
It was just an arrow in the main, just a game.

186
00:00:744
I mean, it's cool to play a game and you can't look in.

187
00:00:747
And at some point we realized that hey, if you train a large neural network, a very,

188
00:00:752
very large transformer to predict text better and better, something very surprising will

189
00:00:757
happen.

190
00:00:758
The realization also arrived a little bit gradually.

191
00:00:760
We were exploring generative models.

192
00:00:764
We were exploring ideas around next word prediction.

193
00:00:767
Those ideas also related to compression.

194
00:00:770
We were exploring them.

195
00:00:772
Transformer came out.

196
00:00:773
We got really excited.

197
00:00:774
We were like, this is this is the greatest thing.

198
00:00:776
We're going to do transformers now.

199
00:00:777
It's clear to superior than anything else before it.

200
00:00:780
We started transformers with the GPT-1.

201
00:00:783
GPT-1 started to show very interesting signs of life.

202
00:00:786
And that led us to doing GPT-2.

203
00:00:788
And then ultimately GPT-3, GPT-3 really opened everyone else's eyes as well to, hey,

204
00:00:793
this thing has a lot of traction.

205
00:00:795
There is one specific formula right now that everyone is doing.

206
00:00:799
And this formula is train a larger and larger transformer on more and more data.

207
00:00:804
I mean, for me, the big wake up moment to your point was GPT-2, the GPT-3 transition,

208
00:00:808
where you saw such a big step function and capabilities.

209
00:00:812
And then obviously with four open-air eyes published some really interesting research

210
00:00:818
around some of the different domains of knowledge or domains of expertise or train of thought

211
00:00:822
or other things that the models can suddenly do in an emergent form.

212
00:00:825
What was the most surprising thing for you in terms of emergent behavior in these models

213
00:00:829
over time?

214
00:00:830
You know, it's very hard to answer that question.

215
00:00:832
It's very hard to answer because I'm too close and I've seen it progress every step

216
00:00:836
of the way.

217
00:00:838
So as much as I'd like, I find it very hard to answer that question.

218
00:00:842
I think if I had to pick one, I think maybe the most surprising thing.

219
00:00:848
For me, the whole thing works at all.

220
00:00:851
You know, it's hard.

221
00:00:853
I'm not sure I know how to convey this.

222
00:00:856
What I have in mind here because if you see a lot of neural networks do amazing things,

223
00:00:862
obviously neural networks is the thing that works.

224
00:00:865
But I have witnessed personally what it's like to be in a world for many years where the

225
00:00:872
neural networks don't work at all.

226
00:00:874
And then to contrast that to where we are today, just the fact that they work and they

227
00:00:879
do these amazing things.

228
00:00:881
I think maybe the most surprising, the most surprising, if I had to pick one, it would

229
00:00:885
be the fact that when I speak to it, I feel understood.

230
00:00:889
Yeah, there's a really good saying from, I'm trying to remember, maybe it's Arthur Clark

231
00:00:893
or one of the sci-fi authors, which is effectively it says advanced technology is sometimes

232
00:00:899
indistinguishable for magic.

233
00:00:901
Yeah, I'm fully in the scam.

234
00:00:902
Yeah.

235
00:00:904
Definitely feels like there's some magical moments with some of these models now.

236
00:00:907
Is there a way that you guys decide internally given all of the different capabilities you

237
00:00:914
could pursue how to continually choose the set of big projects?

238
00:00:919
You've sort of described that centralization and committing to certain research directions

239
00:00:925
at scale is really important to open AI success.

240
00:00:927
Given the breadth of opportunity now, like what's the process for deciding what's worth

241
00:00:931
working on?

242
00:00:933
I mean, I think there is some combination of bottom up and top down where we have some

243
00:00:938
top down ideas that we believe should work, but we are not 100% sure.

244
00:00:943
So we still need to have good top down ideas and there is a lot of bottom up exploration

245
00:00:948
that's got it, but it will stop down ideas as well.

246
00:00:950
And their combination is what informs us as to what to do next.

247
00:00:955
And if you think about those bottom, I mean, either direction, top down or bottom up ideas,

248
00:00:961
like clearly we have this dominant continue to scale transformers direction.

249
00:00:967
Do you explore additional like architectural directions or is that just not relevant?

250
00:00:972
Certainly possible that the various improvements can be found.

251
00:00:976
I think improvements can be found in all kinds of places, both small improvements and

252
00:00:980
large improvements.

253
00:00:982
I think the way to think about it is that while the current thing that's being done keeps

254
00:00:988
getting better as you keep on increasing the amount of compute and data that you put into

255
00:00:994
it.

256
00:00:995
So we have that property, the bigger you make it, the better it gets.

257
00:00:999
It is also the property that different things get better by different amount as you keep

258
00:00:1006
on improving, as you keep on scaling them up.

259
00:00:1008
So not only one, of course, scale up what we are doing, we also want to keep scaling

260
00:00:1012
up the best thing possible.

261
00:00:1015
What is a, I mean, you probably don't need to predict, because you can see internally,

262
00:00:1020
what do you think is improving most from a capability perspective in the current generation

263
00:00:1025
of scale?

264
00:00:1026
The best way for me to answer this question would be to point out the, to point to the models

265
00:00:1036
that are publicly available.

266
00:00:1038
And you can see how they compare from this year to last year.

267
00:00:1042
And the difference is quite significant.

268
00:00:1044
I'm not talking about the difference between, not only the difference between, let's

269
00:00:1048
say you can look at the difference between GPT 3 and GPT 3.5 and then chat GPT, chat GPT

270
00:00:1053
4, chat GPT 4 with vision.

271
00:00:1056
And you can just see for yourself, it's easy to forget where things used to be, but certainly

272
00:00:1061
the big way in which things are changing is that these models become more and more reliable

273
00:00:1067
before they were very, they were only very partly there.

274
00:00:1071
Right now they are mostly there, but there are still gaps.

275
00:00:1074
And in the future perhaps these models will be there even more.

276
00:00:1078
You could trust their answers, they'll be more reliable, they'll be able to do more

277
00:00:1082
tasks in general across the board.

278
00:00:1085
And then another thing that they will do is that they'll have deeper insight.

279
00:00:1089
As we train them, they gain more and more insight into the true nature of the human world.

280
00:00:1096
And their insight will continue to deepen.

281
00:00:1098
I was just going to ask about how that relates to sort of model scale over time because

282
00:00:1103
a lot of people are really stricken by the capabilities of the very large scale models

283
00:00:1109
and the emergent behavior in terms of understanding of the world.

284
00:00:1111
And then in parallel, as people incorporate some of these things into products, which is

285
00:00:1115
a very different type of path, they often start worrying about inference costs going up

286
00:00:1119
with the scale of the model.

287
00:00:1120
And therefore they're looking for smaller models that are fine-tuned.

288
00:00:1123
But then of course you may lose some of the capabilities around some of the insights

289
00:00:1127
and ability to reason.

290
00:00:1129
And so I was curious in your thinking in terms of how all this evolves over the coming

291
00:00:1132
years.

292
00:00:1133
I would actually point out the main thing that's lost when you switch to the smaller models

293
00:00:1137
is reliability.

294
00:00:1139
I would argue that at this point it is reliability that's the biggest bottleneck to these

295
00:00:1146
models being truly useful.

296
00:00:1147
How are you defining reliability?

297
00:00:1149
So it's like when you ask you the question that's not much harder than other questions

298
00:00:1154
that the model succeeds at.

299
00:00:1157
And you'll have a very high degree of confidence that it will continue to succeed.

300
00:00:1161
So I'll give you an example.

301
00:00:1162
Let's suppose that I want to learn about some historical thing.

302
00:00:1166
And I can ask you what, tell me what is the prevailing opinion about this and about

303
00:00:1170
that and I can keep asking questions.

304
00:00:1172
And let's suppose it answered 20-oomac questions correctly.

305
00:00:1175
I really don't want the 21st question to have a gross mistake.

306
00:00:1181
That's what I mean by reliability.

307
00:00:1182
Or like let's suppose I upload some documents, some financial documents.

308
00:00:1186
Suppose they say something I want you to do some analysis and to make some conclusion

309
00:00:1189
and I want to take action on this basis and this conclusion.

310
00:00:1193
And it's like it's not a super hard task.

311
00:00:1195
And the model, these models clearly succeed on this task most of the time.

312
00:00:1199
But because they don't succeed all the time and if it's a consequential decision I

313
00:00:1202
actually can't trust the model any of those times.

314
00:00:1204
And I have to verify the answer somehow.

315
00:00:1207
So that's how I define reliability.

316
00:00:1208
It's very similar to the self-driving situation, right?

317
00:00:1210
If you have a self-driving car and it's like, does things mostly well?

318
00:00:1216
It's not good enough.

319
00:00:1217
Situation is not as extreme as with a self-driving car, but that's what I mean by reliability.

320
00:00:1221
My perception of reliability is that a, to your point, it goes up with model scale, but

321
00:00:1226
also it goes up and if you fine tune for specific use cases or instances or data sets and so

322
00:00:1232
there is that trade-off in terms of size versus, you know, specialized fine tuning versus

323
00:00:1239
reliability.

324
00:00:1239
So certainly people who care about some specific application have every incentive to give

325
00:00:1245
the smallest model working well enough.

326
00:00:1249
I think that's true.

327
00:00:1250
It's undeniable.

328
00:00:1251
I think anyone who cares about a specific application will want the smallest model for it.

329
00:00:1255
That's self-evident.

330
00:00:1257
I do think though that as models continue to get larger and better, then they will unlock

331
00:00:1262
new and unprecedentedly valuable applications.

332
00:00:1265
So yeah, the small models will have their niche for the less interesting applications,

333
00:00:1269
which are still very useful.

334
00:00:1271
And then the bigger models will be delivering on applications.

335
00:00:1275
Okay, let's pick an example.

336
00:00:1278
Consider the task of producing good legal advice.

337
00:00:1282
It's really valuable if you can really trust the answer.

338
00:00:1284
Maybe you need a much bigger model for it, but it justifies the cost.

339
00:00:1287
There's been a lot of investment this year at the 7B in particular, but 7B, 13B, 34B

340
00:00:1295
sizes.

341
00:00:1296
Do you think continued research at those scales is wasted?

342
00:00:1301
No, of course not.

343
00:00:1303
I mean, I think that in the kind of medium term, medium term by eye-time scale anyway, there

344
00:00:1312
will be an ecosystem that will be different uses for different model sizes.

345
00:00:1318
There will be plenty of people who are very excited for whom the best 7B model is good

346
00:00:1324
enough.

347
00:00:1325
They'll be very happy with it.

348
00:00:1326
And then there'll be plenty of very, very exciting and amazing applications for which

349
00:00:1331
it won't be enough.

350
00:00:1333
I think that's all.

351
00:00:1334
I mean, I think the big models will be better than the small models, but not all applications

352
00:00:1341
will justify the cost of a large model.

353
00:00:1343
What do you think the role of open sources in the ecosystem?

354
00:00:1347
Well, open source is complicated.

355
00:00:1349
I'll describe to you my mental picture.

356
00:00:1352
I think that in the near term, open source is just helping companies produce useful.

357
00:00:1359
Like, let's see.

358
00:00:1360
Why would one want to have an open source, but using open source model instead of a

359
00:00:1365
closed source model that's hosted by some other company?

360
00:00:1368
I mean, I think it's very valid to want to be the final decider on the exact way in

361
00:00:1377
which you want your model to be used.

362
00:00:1379
For you to make the decision of exactly how you want the model to be used and which use

363
00:00:1384
case you wish to support.

364
00:00:1386
And I think there's going to be a lot of demand for open source models.

365
00:00:1388
And I think there will be quite a few companies that will use them.

366
00:00:1392
And I'd imagine that will be the case in the near term.

367
00:00:1394
I would say in the long run, I think the situation with open source models will become

368
00:00:1399
more complicated.

369
00:00:1400
And I'm not sure what the right answer is there.

370
00:00:1403
Right now, it's a little bit difficult to imagine.

371
00:00:1405
So we need to put our future hat.

372
00:00:1409
Maybe future is hat.

373
00:00:1410
It's not too hard to get into a sci-fi mode when you remember that we are talking to

374
00:00:1414
computers and they understand us.

375
00:00:1416
But so far, these models are actually not very competent.

376
00:00:1419
They can't do tasks at all.

377
00:00:1423
I do think that the will come a day where the level of capability of models will be very

378
00:00:1430
high.

379
00:00:1431
One of the day intelligence is power.

380
00:00:1434
Right now, these models, their main impact, I would say, at least popular impact has been

381
00:00:1439
primarily around entertainment and simple questions.

382
00:00:1443
So you talk to a model about this is so cool you produce some images, you had a conversation,

383
00:00:1447
maybe you had some questions, good answer.

384
00:00:1449
But it's very different from completing some large and complicated tasks like what

385
00:00:1456
about if you had a model which could autonomously start and build a large tech company?

386
00:00:1463
I think if these models were open source, they would have it difficult to predict consequence.

387
00:00:1469
Like, we are quite far from these models right now and quite far, I mean by I time scale

388
00:00:1473
the ceiling.

389
00:00:1474
This is not what we're talking about, but the day will come when you have models which

390
00:00:1478
can do science autonomously, like build deliver on big science projects.

391
00:00:1485
And it becomes more complicated as to whether it is desirable that models of such power

392
00:00:1492
should be open source.

393
00:00:1494
I think the argument there is a lot less clear cut, a lot less straightforward compared

394
00:00:1499
to the current level models which are very useful and I think it's fantastic that the current

395
00:00:1504
level models have been built.

396
00:00:1505
So like that is maybe, maybe I answered a slightly bigger question rather than what

397
00:00:1510
is the role of open source models, it was the deal with open source.

398
00:00:1513
And the deal is after a certain capability, it's great, but not difficult to imagine models

399
00:00:1519
sufficiently powerful, which will be built where it becomes a lot less obvious to the benefits

400
00:00:1525
of their open source.

401
00:00:1527
Is there a signal for you that we've reached that level or that we're approaching it?

402
00:00:1531
Like what's the boundary?

403
00:00:1534
So I think figuring out this boundary very well is an urgent research project.

404
00:00:1541
I think one of the things that help is that the close source models are more capable

405
00:00:1549
than open source models.

406
00:00:1550
So the close source models could be studied and so on.

407
00:00:1554
And so you'd have some experience with a generation of close source model and then then

408
00:00:1558
you know, like, oh, these models capabilities, it's fine, there's no big deal there than

409
00:00:1561
in a couple of years, the open source models catch up.

410
00:00:1565
And maybe a day will come and we're going to say, well, like these close source models

411
00:00:1568
they're getting able to drastic and then some other approaches needed.

412
00:00:1573
If we have our, you know, future hat on, maybe it looks like think about like a several

413
00:00:1579
year timeline.

414
00:00:1580
What are the limits you see if any in the in the near term and scaling?

415
00:00:1586
Is it like data, token scarcity, cost of compute, architectural issues?

416
00:00:1592
So the most near term limit to scaling is obviously data.

417
00:00:1597
This is well known and some research is required to address it without going into the details.

418
00:00:1603
I'll just say that the data limit can be overcome and progress will continue.

419
00:00:1610
One question I've heard people debate a little bit.

420
00:00:1613
Is it agreed at which the transformer based models can be applied to sort of the full set

421
00:00:1617
of areas that you'd need for AGI?

422
00:00:1620
And if you look at the human brain, for example, you do have reasonably specialized systems

423
00:00:1625
or all neural networks, be as specialized systems for the visual cortex versus, you know,

424
00:00:1629
areas of higher thought, areas for empathy or other sort of aspects of everything from personality

425
00:00:1634
to processing.

426
00:00:1636
Do you think that the transformer architectures are the main thing that will just keep going

427
00:00:1640
and get us there?

428
00:00:1641
Do you think we'll need other architectures over time?

429
00:00:1643
So I have to, I understand precisely what you're saying and I have to answer to this question.

430
00:00:1650
The first is that in my opinion, the best way to think about the question of architecture

431
00:00:1655
is not in terms of a binary, is it enough?

432
00:00:1659
But how much effort, what will be the cost of using this particular architecture?

433
00:00:1667
Like at this point, I don't think anyone doubts that the transformer architecture can do

434
00:00:1672
amazing things, but maybe something else, maybe some modification could have some computer

435
00:00:1678
efficiency benefits.

436
00:00:1680
So you also better to think about it in terms of computer efficiency rather than in terms

437
00:00:1683
of can it get there at all?

438
00:00:1685
I think at this point, the answer is obviously yes.

439
00:00:1689
To the question about, well, what about the human brain and with its brain regions?

440
00:00:1694
I actually think that the situation there is subtle and deceptive for the following

441
00:00:1701
reasons.

442
00:00:1701
So what I believe you alluded to is the fact that the human brain has known regions.

443
00:00:1706
It has like, it has a speech perception region, it has a speech production region, it

444
00:00:1710
has an image region, it has a face region, it has like all these regions, and it looks

445
00:00:1714
like it's specialized.

446
00:00:1716
But you know what's interesting, sometimes there are cases where very young children have

447
00:00:1722
severe cases of epilepsy at the young age.

448
00:00:1725
And the only way they figured out how to treat such children is by removing half of their

449
00:00:1731
brain.

450
00:00:1733
What was it that happened in such a young age?

451
00:00:1735
These children grow up to be pretty functional adults and they have all the same brain regions

452
00:00:1741
but they are somehow compressed onto one hemisphere.

453
00:00:1745
So maybe some, you know, information processing efficiencies lost, it's a very traumatic

454
00:00:1751
thing to experience, but somehow all these brain regions rearrange themselves.

455
00:00:1753
There is another experiment where that which was done maybe 30 or 40 years ago on ferrets.

456
00:00:1760
For the ferrets is a small animal, it's a pretty mean experiment.

457
00:00:1763
They took the optic nerve of the ferret which comes from its eye and attached it to its

458
00:00:1769
auditory cortex.

459
00:00:1771
So now the inputs from the eye starts to map to the speech processing area of the brain.

460
00:00:1776
And then they recorded different neurons after it had a few days of learning to see and

461
00:00:1781
they found neurons in the auditory cortex, which were very similar to the visual cortex.

462
00:00:1785
All vice versa, it was either they mapped the eye to the ear to the auditory cortex or

463
00:00:1791
the ear to the visual cortex.

464
00:00:1792
But something like this has happened.

465
00:00:1794
These are fairly well known ideas in AI that the cortex of humans and animals are extremely

466
00:00:1800
uniform.

467
00:00:1801
And so that further supports the AI.

468
00:00:1802
You just need one, you need big uniform architecture.

469
00:00:1805
So you need it.

470
00:00:1806
Yeah.

471
00:00:1806
In general, it seems like every biological system is reasonably lazy in terms of taking

472
00:00:1809
one system and then reproducing it and then reusing it in different ways.

473
00:00:1812
And that's true of everything from DNA encoding.

474
00:00:1815
There's 20 amino acids in protein sequences.

475
00:00:1816
And so everything is made out of the same 20 amino acids on through to your point, sort

476
00:00:1822
of how you think about tissue architecture.

477
00:00:1823
So it's remarkable that that carries over into the digital world as well, depending on

478
00:00:1827
the architecture you use.

479
00:00:1828
I mean, the way I see it is that this is an indication from a technological point of

480
00:00:1833
you.

481
00:00:1833
We have very much on the right track because you have all these interesting analogies between

482
00:00:1837
human intelligence and biological intelligence and artificial intelligence.

483
00:00:1841
You got artificial neurons, biological neurons, unified brain architecture for biological intelligence,

484
00:00:1848
unified neural network architecture for artificial intelligence.

485
00:00:1851
At what point do you think we should start thinking about these systems in digital life?

486
00:00:1855
I can answer that question.

487
00:00:1857
I think that will happen when those systems become reliable in such a way as to be very

488
00:00:1863
autonomous.

489
00:00:1864
Right now those systems are clearly not autonomous.

490
00:00:1867
They're inching there, but they're not.

491
00:00:1870
And that makes them a lot less useful too because you can't ask it, hey, like do my homework

492
00:00:1874
or do my taxes or you see what I mean.

493
00:00:1876
So the usefulness is greatly limited.

494
00:00:1878
As the usefulness increases, they will indeed become more like artificial life, which

495
00:00:1883
also makes it more, I would argue, trepidations, right?

496
00:00:1888
Like if you imagine actual artificial life with brains that are smarter than humans, go,

497
00:00:1893
gosh, that's like, that seems pretty monumental.

498
00:00:1897
Why is your definition based on autonomy?

499
00:00:1900
Because if you often look at the definition of biological life, it has to do with reproductive

500
00:00:1905
capability.

501
00:00:1906
Plus, I guess some form of autonomy, right?

502
00:00:1908
Like a virus isn't really necessarily considered alive much of the time, right?

503
00:00:1911
But a bacteria is, and you could imagine situations where you have symbiotic relationships or other

504
00:00:1917
things where something can't really function autonomously, but it's still considered

505
00:00:1920
a life form.

506
00:00:1921
So I'm a little bit curious about autonomy being the definition versus some of these other

507
00:00:1924
aspects.

508
00:00:1926
Well, I mean, definitions are chosen for our convenience and it's a matter of debate.

509
00:00:1931
In my opinion, technology already has the reproduction, the reproductive function, right?

510
00:00:1936
And if you look at, for example, I don't know if you've seen those images of the evolution

511
00:00:1940
of cell phones and then smartphones over the past 25 years, you got this like, what almost

512
00:00:1944
looks like an evolutionary tree or the evolution of cars over the past century.

513
00:00:1948
So technology is already reproducing using the minds of people who copy ideas from

514
00:00:1953
previous generation of technology.

515
00:00:1955
So I claim that the reproduction is already there.

516
00:00:1957
The autonomy piece I claim is not.

517
00:00:1960
And indeed, I also agree that there is no autonomous reproduction.

518
00:00:1963
But that would be like, can you imagine if you have like autonomously reproducing

519
00:00:1967
AI's, I actually think that that is a pretty traumatic and I would say quite a scary thing

520
00:00:1974
if you have an autonomous reproducing AI, if it's also very capable.

521
00:00:1978
Should we talk about super alignment?

522
00:00:1980
Yeah, very much so.

523
00:00:1982
Can you just sort of define it?

524
00:00:1985
And then we were talking about what the boundary is for when you feel we need to begin to worry

525
00:00:1991
about these capabilities being in open source.

526
00:00:1994
Like, what is super alignment and why invest in it now?

527
00:00:1999
The answer to your question really depends to where you think AI is headed.

528
00:00:2006
If you just try to imagine and look into the future, which is of course a very difficult

529
00:00:2011
thing to do.

530
00:00:2012
Let's make let's let's try to do it anyway.

531
00:00:2015
Where do we think things will be in five years or in 10 years?

532
00:00:2019
And progress has been really stunning over the past few years.

533
00:00:2022
Maybe it will be a little bit slower.

534
00:00:2025
But still, if you if you extrapolate this kind of progress,

535
00:00:2027
you'll be in a very, very different place in five years, let alone 10 years.

536
00:00:2034
It doesn't seem implausible.

537
00:00:2036
It doesn't seem at all implausible that we will have computers,

538
00:00:2041
data centers that are much smarter than people.

539
00:00:2044
And by smarter, I don't mean just have more memory or have more knowledge,

540
00:00:2048
but it also have mean have deeper insight into the same subjects that we people are studying

541
00:00:2055
and looking into.

542
00:00:2056
It means learn even faster than people.

543
00:00:2061
Like, what could such a eyes do?

544
00:00:2063
I don't know.

545
00:00:2064
Certainly, if such an AI were the basis of some artificial life, it would be,

546
00:00:2069
well, how do you even think about it?

547
00:00:2071
If you have some very powerful data center that's also alive in a sense,

548
00:00:2076
that's what we're talking about.

549
00:00:2078
And when I imagine this world, my reaction is, gosh,

550
00:00:2081
this is very unpredictable.

551
00:00:2082
What's going to happen?

552
00:00:2083
Very unpredictable.

553
00:00:2084
But the bare minimum, but there is a bare minimum, which we can articulate.

554
00:00:2089
That if such super, if such very, very intelligent, super intelligent data centers

555
00:00:2096
have been built, been built at all,

556
00:00:2098
we want those data centers to hold warm and positive feelings towards people, towards humanity.

557
00:00:2106
Because those, this is going to be non-human life in a sense.

558
00:00:2111
Potentially, it could potentially be that.

559
00:00:2114
And so I would want that any instance of such super intelligence, the warm feelings towards humanity.

560
00:00:2122
And so this is what we are doing with the super alignment project.

561
00:00:2125
You're saying, hey, if you just allow yourself, if you just accept the progress that you've seen,

562
00:00:2132
maybe it will be slower, but it will continue.

563
00:00:2135
If you allow yourself that, then can you start, can start doing productive work today

564
00:00:2143
to build the science so that we will be able to handle the problem of controlling such

565
00:00:2151
future super intelligence of imprinting onto them a strong desire to be nice and kind to people.

566
00:00:2160
Because those data centers, they'll be, they'll be really quite powerful.

567
00:00:2165
You know, there'll probably be many of them that will be, the world will be very complicated.

568
00:00:2169
But somehow, to the extent that they are autonomous, to the extent that they are agents,

569
00:00:2174
to the extent that they are beings, I want them to be, to be pro-social, pro-human social.

570
00:00:2182
That's the goal.

571
00:00:2183
What do you think is the likelihood of that goal? I mean, some of it, it feels like

572
00:00:2190
a, outcome you can, hopefully, affect, right? But are we, are we likely to have pro-social

573
00:00:2197
AIs that we are friends with individually or, you know, as a species?

574
00:00:2202
Well, I mean, friends be, I think, that that part is not necessary.

575
00:00:2207
The, the, the friendship piece, I think, is optional, but I do think that we want to have very

576
00:00:2211
pro-social AI. I think it's, I think it's possible. I don't think it's guaranteed, but I think it's

577
00:00:2216
possible. I think it's going to be possible and the possibility of that will increase in so far

578
00:00:2222
as more and more people allow themselves to look into the future, into the five to ten year future.

579
00:00:2229
And just ask yourself, what, what do you expect AI to be able to do then?

580
00:00:2235
How capable do you expect it to be then? And I think that with each passing year,

581
00:00:2242
if indeed AI continues to improve and as people get to experience, it's going to run

582
00:00:2249
right now, you're talking, making arguments, but if you actually get to experience, oh gosh,

583
00:00:2254
the AI from last year, which was really helpful this year, it puts the previous one to shame,

584
00:00:2260
and you go, okay, and then one year later and one year, it's starting to do science, the AI

585
00:00:2265
software engineer is starting to get really quite good. Let's say, I think that will create a lot

586
00:00:2271
more desire in people for what you just described, for the future superintelligence, to need

587
00:00:2280
be very pro-social. You know, I think it's going to be a lot of disagreement, it's going to be

588
00:00:2284
lots of political questions, but I think that as people see AI actually getting better, as people

589
00:00:2290
experience it, the desire for the pro-social superintelligence, the humanity loving superintelligence,

590
00:00:2299
you know, as much as it can be done, will increase. And on the scientific problem,

591
00:00:2305
you know, I think right now it's still being an area where not that many people were working on.

592
00:00:2310
Our AI is getting powerful enough, you can really start studying productively,

593
00:00:2314
we'll have some very exciting research to share soon.

594
00:00:2320
But I would say that's the big picture situation here. Just really, it's really boils down to,

595
00:00:2325
look at what you've experienced with AI up until now, ask yourself,

596
00:00:2330
like is it slowing down? You look slow down next year, like we will see, and you'll experience it

597
00:00:2335
again and again, and I think it will keep, and what needs to be done, it will keep becoming clear.

598
00:00:2340
Do you think we're just on an accelerated path? Because I think fundamentally, if you look at

599
00:00:2344
certain technology waves, they tend to inflect and then accelerate versus decelerate.

600
00:00:2350
And so it really feels like we're in an acceleration phase right now versus the deceleration phase.

601
00:00:2355
Yeah, I mean, we are, right now it is indeed the case that we are in an acceleration phase.

602
00:00:2359
You know, it's hard to say, you know, multiple forces will come into play. Some forces are

603
00:00:2367
accelerating forces and some forces are decelerating. So for example, the cost and scale are a

604
00:00:2373
decelerating force. The fact that our data is finite is a decelerating force to some degree,

605
00:00:2380
it's that I don't want to overstay. Yeah, it's kind of within an asymptote, right? Like at some

606
00:00:2383
point you had a, but when it's the standard Asker, writers, you might all, well, with the data in

607
00:00:2389
particular, I just think it won't be, it just won't be an issue because we'll figure out some,

608
00:00:2393
something else. But then you might argue, like the size of the engineering project is a decelerating

609
00:00:2399
force, just the complexity of management. On the other hand, the amount of investment is an

610
00:00:2403
accelerating force. The amount of interest from people from engineers, scientists is an

611
00:00:2407
accelerating force. And I think there is one other accelerating force. And that is the fact that

612
00:00:2413
biological evolution has been able to figure it out and the fact that up until now, progress in AI

613
00:00:2421
has had up until this point, the severe property that it's kind of been, you know, it's been very

614
00:00:2427
hard to execute on. But in some sense, it's also been more straightforward than one would have

615
00:00:2433
expected perhaps. Like in some sense, I don't know much physics, but my understanding is that if

616
00:00:2441
you want to make progress in quantum physics or something, you need to be really intelligent and

617
00:00:2448
spend many years in grad school studying how these things work. Whereas with AI, if people come in,

618
00:00:2455
get up to speed quickly, start making contributions quickly. It has the flavor is somehow different,

619
00:00:2459
somehow it's very, there is some kind of, there's a lot of give to this particular research.

620
00:00:2465
And I think this is also an accelerating force. How will it all play out remains to be seen?

621
00:00:2470
Like it may be that somehow the scale required that engineering complexity will start to make it so

622
00:00:2475
that the rate of progress will start to slow down. It will still continue, but maybe not as quick as

623
00:00:2479
we had before. Or maybe the forces which are coming together to push it will be such that it will

624
00:00:2485
be as fast for maybe a few more years before it will start to slow down. If at all, that's that will be

625
00:00:2491
my articulation here. Ilya, this has been a great conversation. Thanks for your hands. Thank you so

626
00:00:2497
much for the conversation. I really enjoyed it. Find us on Twitter at No Pryor's Pod. Subscribe to

627
00:00:2502
our YouTube channel if you want to see our faces. Follow the show on Apple podcasts, Spotify,

628
00:00:2507
or wherever you listen. That way you get a new episode every week. And sign up for emails or

629
00:00:2512
find transcripts for every episode at No-Pryors.com.

