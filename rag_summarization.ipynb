{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting up the text into smaller chunks for indexing\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200, #striding over the text\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "with open('transcript.txt','r') as f:\n",
    "    transcript = f.read()\n",
    "    texts = text_splitter.split_text(transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method OpenAIEmbeddings.embed_query of OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization='', allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = FAISS.from_texts(texts, embeddings)\n",
    "docsearch.embedding_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "# set up FAISS as a generic retriever \n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
    "\n",
    "# create the chain to answer questions \n",
    "rqa = RetrievalQA.from_chain_type(llm=OpenAI(), \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The transcript is about Artificial Intelligence (AI) and the progress it has made. It discusses how AI will affect us in the future and how people are working together to ensure that AI will never go rogue. It begins from 0:00:04 to 0:02:57 discussing the progress of AI and how it is becoming widespread. From 0:02:58 to 0:10:41, the speaker talks about how he got into AI and how he is working to ensure that AI will not go rogue. From 0:10:43 to 0:09:37 the speaker discusses the excitement and investment in the AI field.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the main content of the transcript and summarize it using the the timestamps of each important section in the transcript\"\n",
    "rqa(query)['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Define a new Pydantic model with field descriptions and tailored for Twitter.\n",
    "class TranscriptionChapter(BaseModel):\n",
    "    summarization: str = Field(description=\"A short summarization of the transcript\")\n",
    "    chapter: List[str] = Field(description=\"List of main contents in the transcript\")\n",
    "    time_stamps: List[str] = Field(description=\"List of timestamps based on the main contents\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the parser with the new model.\n",
    "output_parser = PydanticOutputParser(pydantic_object=TranscriptionChapter)\n",
    "\n",
    "# Update the prompt to match the new query and desired format.\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{question} in chapters and also mark the chapter with the correct timestamps from the transcript\"\n",
    "        )\n",
    "    ],\n",
    "    input_variables=['question'],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": output_parser.get_format_instructions(),\n",
    "    },\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "_input = prompt.format_prompt(question='Summarize the transcript')\n",
    "\n",
    "# create the chain to answer questions \n",
    "rqa = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever,\n",
    "                                  return_source_documents=True)\n",
    "type(_input.to_messages())\n",
    "#rqa(_input.to_messages())['result']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_gpt",
   "language": "python",
   "name": "local_gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
